{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL8MYMqxgBAFnmkzspYMau",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SofiaLebronPR/Linguistic_Mini_Project/blob/main/Mini_Linguistic_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workshop: Mini Linguistics Research — Live Survey Analysis**\n",
        "\n",
        "This notebook analyzes student survey responses collected during the presentation.\n",
        "Steps you'll see: load CSV → clean text → quick counts (deixis, emojis) → tiny sentiment → simple visuals."
      ],
      "metadata": {
        "id": "zSQlK6p1Ru1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Import Libraries**\n"
      ],
      "metadata": {
        "id": "GTXpW1r1ZM50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re, string\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AmCY2PtSR5wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Load CSV file**"
      ],
      "metadata": {
        "id": "hdOmrRUKdazn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your exported CSV (Google Forms)\n",
        "CSV_PATH = r\"C:.csv\""
      ],
      "metadata": {
        "id": "r8gkeE8hdrcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Read the file**"
      ],
      "metadata": {
        "id": "xPHUkIXqd4mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5k3JvDJMd9GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Filter Consent**"
      ],
      "metadata": {
        "id": "BAmn88VeSLve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only students who consented (Yes)\n",
        "if \"consent\" in df.columns:\n",
        "    df = df[df[\"consent\"].astype(str).str.strip().str.lower().isin([\"yes\",\"y\",\"sí\",\"si\"])]\n",
        "else:\n",
        "    print(\"Column 'consent' not found — skipping filter.\")"
      ],
      "metadata": {
        "id": "p930xqg7SN3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Select Comment Column**"
      ],
      "metadata": {
        "id": "hJEu_cdgeJzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure text column exists\n",
        "TEXT_COL = \"comment_text\"\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Expected a '{TEXT_COL}' column with student comments.\")\n"
      ],
      "metadata": {
        "id": "Oek1XS3IeQ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Normalize text**"
      ],
      "metadata": {
        "id": "7YR4QaW_eo5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
        "print(\"Rows after consent filter:\", len(df))\n",
        "df[[TEXT_COL]].head()"
      ],
      "metadata": {
        "id": "0wnPwvDQes-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Tokenization & Emoji Extraction**"
      ],
      "metadata": {
        "id": "qOGsD8yHSjYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenization (regex split, keep emojis separate)\n",
        "EMOJI_RE = re.compile(r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\")\n",
        "WORD_RE = re.compile(r\"[A-Za-zÁÉÍÓÚÜÑáéíóúüñ']+\")\n",
        "\n",
        "# Extract emojis from text\n",
        "def extract_emojis(text):\n",
        "    return EMOJI_RE.findall(text)\n",
        "# Identifies each word with a token\n",
        "def tokenize_words(text):\n",
        "    return WORD_RE.findall(text)\n",
        "# Normalize text (so everything \"looks the same\")\n",
        "def normalize(text):\n",
        "    return text.lower().strip()"
      ],
      "metadata": {
        "id": "v_HpeAY8Sg94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Building the Token Lists**"
      ],
      "metadata": {
        "id": "fD-6wxkcSrpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_emojis = []\n",
        "all_words = []\n",
        "\n",
        "for t in df[TEXT_COL]:\n",
        "    t_norm = normalize(t)\n",
        "    all_emojis.extend(extract_emojis(t_norm))\n",
        "    all_words.extend([w.lower() for w in tokenize_words(t_norm)])"
      ],
      "metadata": {
        "id": "lCrE7mh4Swss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Counting Words, Emojis, and Bigrams**"
      ],
      "metadata": {
        "id": "lGU0f1aXgasV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(all_words)\n",
        "emoji_counts = Counter(all_emojis)\n",
        "\n",
        "# Print the Top 10 words so we can see them\n",
        "print(\"Top 10 words:\", word_counts.most_common(10))\n",
        "print(\"Top emojis:\", emoji_counts.most_common(10))\n",
        "\n",
        "# Simple bigrams\n",
        "bigrams = Counter(zip(all_words, all_words[1:]))\n",
        "print(\"Top 10 bigrams:\", bigrams.most_common(10))"
      ],
      "metadata": {
        "id": "_bKoGYRKgsj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Tiny Sentiment Analysis with mini  lexicon-based, EN + ES — toy model for class**"
      ],
      "metadata": {
        "id": "o_-H6Wk7S1lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal bilingual sentiment wordlists (we can add more in class!)\n",
        "pos_words = {\n",
        "    \"good\",\"great\",\"funny\",\"hopeful\",\"uplifting\",\"love\",\"cool\",\"nice\",\"wow\",\n",
        "    \"bueno\",\"buenisimo\",\"buenísimo\",\"gracioso\",\"divertido\",\"chévere\",\"nítido\",\"mejor\",\"feliz\",\"alegre\",\"esperanza\"\n",
        "}\n",
        "neg_words = {\n",
        "    \"bad\",\"sad\",\"angry\",\"harsh\",\"fake\",\"worse\",\"hate\",\"mad\",\"annoying\",\"toxic\",\n",
        "    \"malo\",\"triste\",\"enojado\",\"falso\",\"peor\",\"odio\",\"molesto\",\"tóxico\",\"preocupación\",\"preocupado\",\"miedo\"\n",
        "}\n",
        "\n",
        "# A funtion to a count for the sentiment score\n",
        "def sentiment_score(text):\n",
        "    toks = [w.lower() for w in tokenize_words(text)]\n",
        "    score = 0\n",
        "    for w in toks:\n",
        "        if w in pos_words:\n",
        "            score += 1\n",
        "        if w in neg_words:\n",
        "            score -= 1\n",
        "    return score\n",
        "\n",
        "df[\"sentiment_score\"] = df[TEXT_COL].apply(sentiment_score)\n",
        "print(df[[\"sentiment_score\", TEXT_COL]].head())"
      ],
      "metadata": {
        "id": "PLPzfO-XS5n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Mini Deixis / Indexicals Analysis (EN + ES pronouns/determiners)**"
      ],
      "metadata": {
        "id": "1Mv0dTBFTBgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deixis_terms = {\n",
        "    \"self\":[\"i\",\"me\",\"my\",\"yo\",\"mí\",\"mi\",\"mío\",\"mía\"],\n",
        "    \"group_in\":[\"we\",\"us\",\"our\",\"nosotros\",\"nosotras\",\"nuestro\",\"nuestra\"],\n",
        "    \"group_out\":[\"they\",\"them\",\"their\",\"ellos\",\"ellas\",\"su\",\"sus\"],\n",
        "    \"proximal\":[\"this\",\"here\",\"aquí\",\"este\",\"esta\",\"esto\"],\n",
        "    \"distal\":[\"that\",\"there\",\"allí\",\"ese\",\"esa\",\"eso\",\"allá\"]\n",
        "}\n",
        "#Funtction to identify and acount for deixis/indexical\n",
        "def count_deixis(text):\n",
        "    toks = [w.lower() for w in tokenize_words(text)]\n",
        "    counts = {k:0 for k in deixis_terms}\n",
        "    for k, vocab in deixis_terms.items():\n",
        "        counts[k] = sum(1 for w in toks if w in vocab)\n",
        "    return counts\n",
        "# How to present the results\n",
        "deixis_results = df[TEXT_COL].apply(count_deixis)\n",
        "deixis_df = pd.DataFrame(list(deixis_results))\n",
        "summary_deixis = deixis_df.sum().sort_values(ascending=False)\n",
        "summary_deixis"
      ],
      "metadata": {
        "id": "fMyNPzedTFLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Language mixing (Code-Switching Ratio) (very rough): ES vs EN stopword hits**"
      ],
      "metadata": {
        "id": "IBVhP3fPTKAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_sw = {\"el\",\"la\",\"los\",\"las\",\"de\",\"y\",\"que\",\"en\",\"a\",\"un\",\"una\",\"yo\",\"nosotros\",\"aquí\",\"este\",\"esa\",\"eso\",\"para\",\"por\"}\n",
        "en_sw = {\"the\",\"and\",\"that\",\"in\",\"to\",\"a\",\"we\",\"i\",\"here\",\"this\",\"that\",\"for\",\"of\"}\n",
        "# Funtion to identify and acount for code-switching\n",
        "def lang_mix(text):\n",
        "    toks = [w.lower() for w in tokenize_words(text)]\n",
        "    es = sum(1 for w in toks if w in es_sw)\n",
        "    en = sum(1 for w in toks if w in en_sw)\n",
        "    return pd.Series({\"es_hits\":es, \"en_hits\":en, \"mix_ratio\": (min(es,en) / max(1, max(es,en)) )})\n",
        "# How to present the results\n",
        "df_lang = df[TEXT_COL].apply(lang_mix)\n",
        "df = pd.concat([df, df_lang], axis=1)\n",
        "df[[\"es_hits\",\"en_hits\",\"mix_ratio\",\"sentiment_score\"]].head()"
      ],
      "metadata": {
        "id": "36EJn_FzTXaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. Visuals**"
      ],
      "metadata": {
        "id": "lr60RR_JTbXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **a) Word Frequency**"
      ],
      "metadata": {
        "id": "Yf5pQHA2TxlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 words\n",
        "wc10 = Counter([w for w in tokenize_words(\" \".join(df[TEXT_COL].astype(str))) if len(w)>2]).most_common(10)\n",
        "words, counts = zip(*wc10) if wc10 else ([], [])\n",
        "plt.figure()\n",
        "plt.bar(words, counts)\n",
        "plt.title(\"Top 10 Words\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H6BmO4OMTglO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b) Sentiment Distribution**"
      ],
      "metadata": {
        "id": "-YcPo8-AT28A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution\n",
        "plt.figure()\n",
        "df[\"sentiment_score\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "plt.title(\"Sentiment Score Distribution\")\n",
        "plt.xlabel(\"score\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ghUysVUXT7Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **c) Deixis/Indexicals**"
      ],
      "metadata": {
        "id": "JS-VRZYYUQdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deixis summary (bar)\n",
        "plt.figure()\n",
        "summary = pd.Series({\"self\":0,\"group_in\":0,\"group_out\":0,\"proximal\":0,\"distal\":0})\n",
        "try:\n",
        "    # 'summary_deixis' defined earlier\n",
        "    summary = summary.add(summary_deixis, fill_value=0)\n",
        "except NameError:\n",
        "    pass\n",
        "summary = summary.sort_values(ascending=False)\n",
        "summary.plot(kind=\"bar\")\n",
        "plt.title(\"Deixis/Indexicals (Total Counts)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AP4JNZIPUV89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **14. (Optional) Simple KWIC — quick concordance**"
      ],
      "metadata": {
        "id": "oDIc40R3UrKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kwic(df, col, keyword, window=5, n=10):\n",
        "    out = []\n",
        "    kw = keyword.lower()\n",
        "    for text in df[col].astype(str):\n",
        "        toks = [w.lower() for w in re.findall(r\"[A-Za-zÁÉÍÓÚÜÑáéíóúüñ']+\", text)]\n",
        "        for i, w in enumerate(toks):\n",
        "            if w == kw:\n",
        "                left = \" \".join(toks[max(0, i-window):i])\n",
        "                right = \" \".join(toks[i+1:i+1+window])\n",
        "                out.append((left, w, right))\n",
        "                if len(out) >= n:\n",
        "                    return out\n",
        "    return out\n",
        "\n",
        "for left, w, right in kwic(df, \"comment_text\", keyword=\"nosotros\", window=4, n=5):\n",
        "    print(f\"{left} [{w}] {right}\")"
      ],
      "metadata": {
        "id": "Ock1mWuIUsB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}